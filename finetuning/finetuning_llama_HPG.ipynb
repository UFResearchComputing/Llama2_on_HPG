{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd97224-3255-4a86-b7ab-8afdb9d28b8a",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama2 on HiPerGator\n",
    "\n",
    "In this notebook, we will guide you through the process of fine-tuning the Llama2 model on HiPerGator using a single GPU, multiple GPUs, and multiple nodes with job scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c35033-68a3-427f-aa6b-e7b85b3375d1",
   "metadata": {},
   "source": [
    "## 1. Fine-tuning with Single GPU\n",
    "This recipe steps you through how to finetune a Llama 2 model on the text summarization task using the [samsum](https://huggingface.co/datasets/samsum) dataset on a single GPU.\n",
    "\n",
    "These are the instructions for using the canonical [finetuning script](../src/finetuning.py) in the llama-recipes package.\n",
    "\n",
    "\n",
    "### 1.1 Requirements\n",
    "\n",
    "Ensure that you have installed the llama-recipes package ([details](https://github.com/meta-llama/llama-recipes)).\n",
    "\n",
    "To run fine-tuning on a single GPU, we will make use of two packages:\n",
    "1. [PEFT](https://github.com/huggingface/peft) to use parameter-efficient finetuning.\n",
    "2. [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for int8 quantization.\n",
    "\n",
    "\n",
    "### 1.2 How to run it?\n",
    "\n",
    "```bash\n",
    "python finetuning.py  --use_peft --peft_method lora --quantization --use_fp16 --model_name /path_of_model_folder/7B --output_dir Path/to/save/PEFT/model\n",
    "```\n",
    "\n",
    "Here is an example of how you can fine-tune the Llama2 model on a single GPU using a job script. \n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #FF0000;border-left-width: 10px;background-color: #fff\">\n",
    "    <strong>Warning:</strong> Remember to update the path to your own directory accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102afac-3ef1-4077-909d-11b6e7e2de28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 1 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_1gpu.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361db601-8b6f-4e34-95a2-f4b0bafa0c44",
   "metadata": {},
   "source": [
    "The args used in the command above are:\n",
    "\n",
    "* `--use_peft` boolean flag to enable PEFT methods in the script\n",
    "* `--peft_method` to specify the PEFT method, here we use `lora` other options are `llama_adapter`, `prefix`.\n",
    "* `--quantization` boolean flag to enable int8 quantization\n",
    "\n",
    "> **Note:** If you are using a multi-GPU machine, please ensure that only one of them is visible by setting `export CUDA_VISIBLE_DEVICES=GPU:id`.\n",
    " \n",
    "#### 1.2.1 How to run with different datasets?\n",
    "\n",
    "Currently 3 open source datasets are supported that can be found in [Datasets config file](../src/configs/datasets.py). You can also use your custom dataset (more info [here](./datasets/README.md)).\n",
    "\n",
    "* `grammar_dataset` : use this [notebook](../src/datasets/grammar_dataset/grammar_dataset_process.ipynb) to pull and process the Jfleg and C4 200M datasets for grammar checking.\n",
    "\n",
    "* `alpaca_dataset` : to get this open source data please download the `aplaca.json` to `dataset` folder.\n",
    "\n",
    "```bash\n",
    "wget -P ../src/datasets https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
    "```\n",
    "\n",
    "* `samsum_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80c17a-c393-4d40-81a5-f3da7031fd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -P ../src/datasets https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7087147-4eb6-42f1-9318-db7b87a5592b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 1 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_1gpu_grammar.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add5d98-c368-4183-b5db-59e50af47889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 1 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_1gpu_alpaca.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf7c02-7c91-403c-83db-b97b35ed908b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 1 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_1gpu_samsum.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae77f42-5fb8-4b3e-b872-790938b3e8b5",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning with Multi GPU\n",
    "This recipe steps you through how to finetune a Llama 2 model on the text summarization task using the [samsum](https://huggingface.co/datasets/samsum) dataset on multiple GPUs in a single or across multiple nodes.\n",
    "\n",
    "\n",
    "### 2.1 Requirements\n",
    "Ensure that you have installed the llama-recipes package ([details](https://github.com/meta-llama/llama-recipes)).\n",
    "\n",
    "We will also need 2 packages:\n",
    "1. [PEFT](https://github.com/huggingface/peft) to use parameter-efficient finetuning.\n",
    "2. [FSDP](https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html) which helps us parallelize the training over multiple GPUs. [More details](./LLM_finetuning_overview.md#2-full-partial-parameter-finetuning).\n",
    "\n",
    "> **Note** \n",
    "> The llama-recipes package will install PyTorch 2.0.1 version. In case you want to use FSDP with PEFT for multi GPU finetuning, please install the PyTorch nightlies ([details](https://github.com/meta-llama/llama-recipes?tab=readme-ov-file#pytorch-nightlies))\n",
    ">\n",
    "> INT8 quantization is not currently supported in FSDP\n",
    "\n",
    "\n",
    "### 2.2 How to run it\n",
    "Get access to a machine with multiple GPUs (in this case we tested with 4 A100 and A10s).\n",
    "\n",
    "#### With FSDP + PEFT\n",
    "\n",
    "\n",
    "**Single-node Multi-GPU**\n",
    "\n",
    "```bash\n",
    "torchrun --nnodes 1 --nproc_per_node 4  finetuning.py --enable_fsdp --model_name /path_of_model_folder/7B --use_peft --peft_method lora --output_dir Path/to/save/PEFT/model\n",
    "\n",
    "```\n",
    "\n",
    "Here is an example of how you can fine-tune the Llama2 model on multiple GPUs using a job script. \n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #FF0000;border-left-width: 10px;background-color: #fff\">\n",
    "    <strong>Warning:</strong> Remember to update the path to your own directory accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ff32c-8f80-43d3-b5d2-d2bd8300278e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on multiple GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_4gpu.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92edd5d-c7be-455c-822c-4580984ef87a",
   "metadata": {},
   "source": [
    "**Multi-node Multi-GPU**\n",
    "Here we use a slurm script to schedule a job with slurm over multiple nodes.\n",
    "```bash  \n",
    "# Change the num nodes and GPU per nodes in the script before running.\n",
    "sbatch ./multi_node.sh\n",
    "```\n",
    "We use `torchrun` to spawn multiple processes for FSDP.\n",
    "\n",
    "The args used in the command above are:\n",
    "* `--enable_fsdp` boolean flag to enable FSDP  in the script\n",
    "* `--use_peft` boolean flag to enable PEFT methods in the script\n",
    "* `--peft_method` to specify the PEFT method, here we use `lora` other options are `llama_adapter`, `prefix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7757929-2d27-4c2c-bd70-76f8483bc3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on multiple GPUs: <font color=\"green\">sbatch ./code/multi_node.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d609d-7fba-442c-b9f8-ffebc13aba05",
   "metadata": {},
   "source": [
    "#### With only FSDP\n",
    "If interested in running full parameter finetuning without making use of PEFT methods, please use the following command. Make sure to change the `nproc_per_node` to your available GPUs. This has been tested with `BF16` on 8xA100, 40GB GPUs.\n",
    "\n",
    "```bash\n",
    "torchrun --nnodes 1 --nproc_per_node 8  finetuning.py --enable_fsdp --model_name /path_of_model_folder/7B --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --pure_bf16 --use_fast_kernels\n",
    "```\n",
    "\n",
    "#### Using less CPU memory (FSDP on 70B model)\n",
    "\n",
    "If you are running full parameter fine-tuning on the 70B model, you can enable `low_cpu_fsdp` mode as the following command. This option will load model on rank0 only before moving model to devices to construct FSDP. This can dramatically save cpu memory when loading large models like 70B (on a 8-gpu node, this reduces cpu memory from 2+T to 280G for 70B model). This has been tested with `BF16` on 16xA100, 80GB GPUs.\n",
    "\n",
    "```bash\n",
    "torchrun --nnodes 1 --nproc_per_node 8 finetuning.py --enable_fsdp --low_cpu_fsdp --pure_bf16 --model_name /path_of_model_folder/70B --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc9b7e-6a44-41a6-9568-d3ab0398600a",
   "metadata": {},
   "source": [
    "### 2.4 Running with different datasets\n",
    "Currently 3 open source datasets are supported that can be found in [Datasets config file](../src/configs/datasets.py). You can also use your custom dataset (more info [here](./datasets/README.md)).\n",
    "\n",
    "* `grammar_dataset` : use this [notebook](../src/datasets/grammar_dataset/grammar_dataset_process.ipynb) to pull and process the Jfleg and C4 200M datasets for grammar checking.\n",
    "\n",
    "* `alpaca_dataset` : to get this open source data please download the `aplaca.json` to `dataset` folder.\n",
    "\n",
    "```bash\n",
    "wget -P ../../src/llama_recipes/datasets https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
    "```\n",
    "\n",
    "* `samsum_dataset`\n",
    "\n",
    "To run with each of the datasets set the `dataset` flag in the command as shown below:\n",
    "\n",
    "```bash\n",
    "# grammer_dataset\n",
    "torchrun --nnodes 1 --nproc_per_node 4  finetuning.py --enable_fsdp  --model_name /path_of_model_folder/7B --use_peft --peft_method lora --dataset grammar_dataset --save_model --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned  --pure_bf16 --output_dir Path/to/save/PEFT/model\n",
    "\n",
    "# alpaca_dataset\n",
    "\n",
    "torchrun --nnodes 1 --nproc_per_node 4  finetuning.py --enable_fsdp  --model_name /path_of_model_folder/7B --use_peft --peft_method lora --dataset alpaca_dataset --save_model --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --pure_bf16 --output_dir Path/to/save/PEFT/model\n",
    "\n",
    "\n",
    "# samsum_dataset\n",
    "\n",
    "torchrun --nnodes 1 --nproc_per_node 4  finetuning.py --enable_fsdp --model_name /path_of_model_folder/7B --use_peft --peft_method lora --dataset samsum_dataset --save_model --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --pure_bf16 --output_dir Path/to/save/PEFT/model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f472ca1-0650-4f0c-a8ad-a0d8269491b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 4 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_4gpu_grammar.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5453415-07da-49fc-929a-ab7835a99602",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 4 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_4gpu_alpaca.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2b117-e661-4fb7-8f2b-42e7d8433cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session by executing: <font color=\"green\">cd /data/ai/tutorial/Llama2_on_HPG/finetuning</font>\n",
    "   Step 3: Run the Llama2 pretraining on 4 GPUs: <font color=\"green\">sbatch ./code/finetuning_llama_4gpu_samsum.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803a2c4-fec3-4937-a8a7-d9e9fd654399",
   "metadata": {},
   "source": [
    "### 2.5 [TIP] Slow interconnect between nodes?\n",
    "In case you are dealing with slower interconnect network between nodes, to reduce the communication overhead you can make use of `--hsdp` flag. \n",
    "\n",
    "HSDP (Hybrid sharding Data Parallel) helps to define a hybrid sharding strategy where you can have FSDP within `sharding_group_size` which can be the minimum number of GPUs you can fit your model and DDP between the replicas of the model specified by `replica_group_size`.\n",
    "\n",
    "This will require to set the Sharding strategy in [fsdp config](../src/configs/fsdp.py) to `ShardingStrategy.HYBRID_SHARD` and specify two additional settings, `sharding_group_size` and `replica_group_size` where former specifies the sharding group size, number of GPUs that you model can fit into to form a replica of a model and latter specifies the replica group size, which is world_size/sharding_group_size.\n",
    "\n",
    "```bash\n",
    "\n",
    "torchrun --nnodes 4 --nproc_per_node 8 ./finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --model_name /path_of_model_folder/70B --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --hsdp --sharding_group_size n --replica_group_size world_size/n\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514377c-d33b-497b-a466-01eb4b6a1ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
